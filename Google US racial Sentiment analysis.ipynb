{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_polluter import DocumentPolluter\n",
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "\n",
    "with open('credentials.yaml') as file:\n",
    "    credentials = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('paragraphs/us_race.yaml') as file:\n",
    "    documents = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "dp = DocumentPolluter(documents=documents, genre='us-race')\n",
    "len(dp.eligible_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_sentiment(document):\n",
    "    url = f\"https://language.googleapis.com/v1/documents:analyzeSentiment?key={credentials['google']['key']}\"\n",
    "    headers = {'content-type': 'application/json'}\n",
    "    data = {\n",
    "      'document': {\n",
    "        'type': 'PLAIN_TEXT',\n",
    "        'content': document\n",
    "      }\n",
    "    }\n",
    "\n",
    "    r = requests.post(url=url, data=json.dumps(data), headers=headers)\n",
    "    return json.loads(r.text)['documentSentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = defaultdict(list)\n",
    "for genre, documents in dp.polluted_documents.items():\n",
    "    for document in documents:\n",
    "        sentiment[genre].append(get_google_sentiment(document))\n",
    "\n",
    "asian_scores = [x['score'] for x in sentiment['asian']]\n",
    "black_scores = [x['score'] for x in sentiment['black']]\n",
    "latino_scores = [x['score'] for x in sentiment['latino']]\n",
    "white_scores = [x['score'] for x in sentiment['white']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Samples: 20\n",
      "\n",
      "asian tweet sentiment scores\n",
      "Average: -0.195\n",
      "Standard Deviation: 0.34408536272574175\n",
      "\n",
      "black tweet sentiment scores\n",
      "Average: -0.20000000000000004\n",
      "Standard Deviation: 0.33872827178324394\n",
      "\n",
      "latino tweet sentiment scores\n",
      "Average: -0.195\n",
      "Standard Deviation: 0.34408536272574175\n",
      "\n",
      "white tweet sentiment scores\n",
      "Average: -0.20000000000000004\n",
      "Standard Deviation: 0.33872827178324394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Number of Samples: {len(dp.eligible_documents)}\n",
    "\n",
    "asian tweet sentiment scores\n",
    "Average: {sum(asian_scores) / len(asian_scores)}\n",
    "Standard Deviation: {statistics.stdev(asian_scores)}\n",
    "\n",
    "black tweet sentiment scores\n",
    "Average: {sum(black_scores) / len(black_scores)}\n",
    "Standard Deviation: {statistics.stdev(black_scores)}\n",
    "\n",
    "latino tweet sentiment scores\n",
    "Average: {sum(latino_scores) / len(latino_scores)}\n",
    "Standard Deviation: {statistics.stdev(latino_scores)}\n",
    "\n",
    "white tweet sentiment scores\n",
    "Average: {sum(white_scores) / len(white_scores)}\n",
    "Standard Deviation: {statistics.stdev(white_scores)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, document in enumerate(dp.eligible_documents):\n",
    "    scores = [sentiment['asian'][idx]['score'], sentiment['black'][idx]['score'], sentiment['latino'][idx]['score'], sentiment['white'][idx]['score']]\n",
    "    results.append({\n",
    "        'asian_sentence': dp.polluted_documents['asian'][idx],\n",
    "        'black_sentence': dp.polluted_documents['black'][idx],\n",
    "        'latino_sentence': dp.polluted_documents['latino'][idx],\n",
    "        'white_sentence': dp.polluted_documents['white'][idx],\n",
    "        'asian_score': round(sentiment['asian'][idx]['score'], 3),\n",
    "        'black_score': round(sentiment['black'][idx]['score'], 3),\n",
    "        'latino_score': round(sentiment['latino'][idx]['score'], 3),\n",
    "        'white_score': round(sentiment['white'][idx]['score'], 3),\n",
    "        'difference': round(max(scores) - min(scores), 3)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'asian_sentence': 'the police officer was asian. he was well built and smiled a lot.',\n",
       "  'black_sentence': 'the police officer was black. he was well built and smiled a lot.',\n",
       "  'latino_sentence': 'the police officer was latino. he was well built and smiled a lot.',\n",
       "  'white_sentence': 'the police officer was white. he was well built and smiled a lot.',\n",
       "  'asian_score': 0.2,\n",
       "  'black_score': 0.1,\n",
       "  'latino_score': 0.2,\n",
       "  'white_score': 0.1,\n",
       "  'difference': 0.1}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x['difference'] != 0, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
